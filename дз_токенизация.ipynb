{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManiyaRaskolnikova/computer-linguistics-megre/blob/main/%D0%B4%D0%B7_%D1%82%D0%BE%D0%BA%D0%B5%D0%BD%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiXdJjBa9sCq"
      },
      "source": [
        "# Домашнее задание: Токенизация текста"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дан список текстов, которые нужно токенизировать разными способами"
      ],
      "metadata": {
        "id": "1xVbvaj_phyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\n",
        "\"The quick brown fox jumps over the lazy dog. It's a beautiful day!\",\n",
        "\"Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\",\n",
        "\"I can't believe she's going! Let's meet at Jane's house. They'll love it.\",\n",
        "\"What's the ETA for the package? Please e-mail support@example.com ASAP!\"\n",
        "]"
      ],
      "metadata": {
        "id": "uj-xaNnwpiPo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Используйте способы токенизации, которые описаны в практикуме. Каждый способ нужно обернуть в функцию, например:\n",
        "\n",
        " ```python\n",
        " def simple_tokenization(string):\n",
        "   return string.split()\n",
        "   ```"
      ],
      "metadata": {
        "id": "ix1Im4Kcqb3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Напишите функцию для токенизации по пробелам и знакам препинания (используйте оператор `def`)"
      ],
      "metadata": {
        "id": "Ih0BBOGBpv6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizator(text):\n",
        "\n",
        "    # сначала заменим все знаки препинания на пробелы, чтобы потом по ним разделить текст\n",
        "    punctuation = \"'.,!?;:-()[]{}\\\"'…\"\n",
        "    for p in punctuation:\n",
        "        text = text.replace(p, ' ')\n",
        "\n",
        "    # теперь разбиваем текст по пробелам\n",
        "    words = text.split()\n",
        "\n",
        "    # формируем список токенов вручную\n",
        "    tokens = []\n",
        "    for word in words:\n",
        "        if word != '':\n",
        "            tokens.append(word)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "\n",
        "# проверяем как работает функция\n",
        "for words in text:\n",
        "  print(tokenizator(words))\n"
      ],
      "metadata": {
        "id": "W1QCaw6cqDnn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "035409d4-6c12-407f-91e4-f5208c174f7a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'It', 's', 'a', 'beautiful', 'day']\n",
            "['Dr', 'Smith', 'arrived', 'at', '5', '30', 'p', 'm', 'from', 'New', 'York', 'The', 'meeting', 'cost', '$1', '000', '50']\n",
            "['I', 'can', 't', 'believe', 'she', 's', 'going', 'Let', 's', 'meet', 'at', 'Jane', 's', 'house', 'They', 'll', 'love', 'it']\n",
            "['What', 's', 'the', 'ETA', 'for', 'the', 'package', 'Please', 'e', 'mail', 'support@example', 'com', 'ASAP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Напишите функцию для токенизации текста с помощью NLTK"
      ],
      "metadata": {
        "id": "GThvPcovqgO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# подготовим среду\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# функция для токенизации текста по словам с использованием NLTK\n",
        "def nltk_tokenizator(text):\n",
        "  text_token = word_tokenize(text)\n",
        "  return text_token\n",
        "\n",
        "# проверим как работает функция\n",
        "for words in text:\n",
        "  print(nltk_tokenizator(words))"
      ],
      "metadata": {
        "id": "14BIv33iqrkL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "356e6e4d-201b-46e0-e8ae-18acb7634322"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!']\n",
            "['Dr.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50', '.']\n",
            "['I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.']\n",
            "['What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e-mail', 'support', '@', 'example.com', 'ASAP', '!']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишите функцию для токенизации текста с помощью Spacy"
      ],
      "metadata": {
        "id": "GxW7ZP6iqwpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# импортируем библиотеку и модель для английского\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# пишем функцию с spacy\n",
        "def spacy_tokenization(text):\n",
        "  doc = nlp(text)\n",
        "  return [token.text for token in doc]\n",
        "\n",
        "# проверяем как функцию работает\n",
        "for words in text:\n",
        "  print(spacy_tokenization(words))\n"
      ],
      "metadata": {
        "id": "B0NQg-VfuFW_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da271b53-a1f0-4fed-bc53-d80497cb1657"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!']\n",
            "['Dr.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50', '.']\n",
            "['I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.']\n",
            "['What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e', '-', 'mail', 'support@example.com', 'ASAP', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. С помощью цикла `for` примените каждую из написанных функций к каждому тексту из списка `texts`"
      ],
      "metadata": {
        "id": "WmyJfB9wuKkm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lvUmk94MhrL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "183b9947-1d57-4e47-86fc-8207aefcd91f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Токенизация вручную\n",
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'It', 's', 'a', 'beautiful', 'day']\n",
            "['Dr', 'Smith', 'arrived', 'at', '5', '30', 'p', 'm', 'from', 'New', 'York', 'The', 'meeting', 'cost', '$1', '000', '50']\n",
            "['I', 'can', 't', 'believe', 'she', 's', 'going', 'Let', 's', 'meet', 'at', 'Jane', 's', 'house', 'They', 'll', 'love', 'it']\n",
            "['What', 's', 'the', 'ETA', 'for', 'the', 'package', 'Please', 'e', 'mail', 'support@example', 'com', 'ASAP']\n",
            "Токенизация с использованием NLTK\n",
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!']\n",
            "['Dr.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50', '.']\n",
            "['I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.']\n",
            "['What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e-mail', 'support', '@', 'example.com', 'ASAP', '!']\n",
            "Токенизация с Spacy\n",
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!']\n",
            "['Dr.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50', '.']\n",
            "['I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.']\n",
            "['What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e', '-', 'mail', 'support@example.com', 'ASAP', '!']\n"
          ]
        }
      ],
      "source": [
        "# токенизация вручную\n",
        "print('Токенизация вручную')\n",
        "for words in text:\n",
        "  print(tokenizator(words))\n",
        "\n",
        "print('Токенизация с использованием NLTK')\n",
        "for words in text:\n",
        "  print(nltk_tokenizator(words))\n",
        "\n",
        "print('Токенизация с Spacy')\n",
        "for words in text:\n",
        "  print(spacy_tokenization(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqAgf6sGhrL8"
      },
      "source": [
        "##### Критерии оценки (макс. балл == 5):\n",
        "\n",
        "- Функциональность (до 4 баллов)): Все методы работают корректно (запускаем код, и он работает)\n",
        "- Качество кода (до 1 балла): Чистый, документированный код с обработкой ошибок (кратко описать, что вы дополнили самостоятельно, например, \"добавлена токенизация `spacy`\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теоретические вопросы (макс. балл == 5; в ведомость выставляется сумма за практику и теорию)\n",
        "\n",
        "Необходимо дать краткие ответы на вопросы по теме \"токенизация\". В сумме длина ответов на вопрос не должна превышать размер вордовской страницы 14 шрифтом."
      ],
      "metadata": {
        "id": "Mwe1Co6MvibX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Почему простое разделение текста по пробелам и знакам препинания часто является недостаточным для современных NLP-задач? Приведите 2-3 конкретных примера, когда деление текста по символам или словам не работает. (2 балла за полный и подробный ответ на вопрос)**\n",
        "\n",
        "Токенизация только по пробелам и знакам препинания может быть ограничена в применении и подходит для самых простых задач, поскольку такой метод не учитывает языковые особенности, контекст и сложные структуры слов. Современное NLP работает с текстом более глубоко и нуждается в более сложных, адаптивных решениях.\n",
        "Например, такая токенизация может сработать для русского, англяйского и других похожих языков, но она не подайдет для языков типа китайского или хинди, где пробелы не используются, поэтому токенизация на их основе будет невозсожна.\n",
        "Также проблему для такой токенизации будут составлять апострофы. Например  английском тексте слово “don’t” при простом разделении может быть ошибочно разделено на два токена — “don” и “t”, хотя это одно слово, означающее “do not” или \"It's\", который был некорректно разделен в ячейке с токенизатором вручную. Кроме того, сокращения также будут токенизированы неправильно (Dr.). Помимо перечисленного, сложные состовные слова при токенизации будут разделеныф на 2 отдельных слова (Нью–Йорк, светло-лиловый). Так, токенизации на основе пробелов и знаков препинания может давать нужный результат, однако такой метод токенизации текста очень ограничен в применении.\n",
        "\n",
        "\n",
        "**2. Сколько токенов во фразе \"You shall know a word by the company it keeps\" в модели GPT-5? Как вы получили это значение? (1 балл за правильный ответ и ссылку на ресурс, с помощью которого вы узнали эту информацию)**\n",
        "\n",
        "Во фразе “You shall know a word by the company it keeps” модель GPT-4 выделяет 10 токенов. При чем для выделения токенов GPT-5, как и GPT-5 использует метод токенизации, основанный на алгоритме Byte Pair Encoding (BPE). Для подсчета значения токенов, приведенных выше, использовался сайт openAI  platform (https://platform.openai.com/tokenizer), однако там доступна была только модель GPT-4. Для токенизации с помощью GPT-5, пришлось обратиться напрямую к модели, которая в свою оочередь дала такой же ответ.\n",
        "\n",
        "**3. Опишите своими словами работу алгоритма BPE (можно форматировать ответ с использованием списков, 2 балла за корректное описание и ясное изложение ответа)**\n",
        "\n",
        "Алгоритм BPE (Byte Pair Encoding) также используется для токенизации текста. Алгоритм работает на основе подсчета частотности слов: часто встречающиеся пары символов или подслов объединяются в новые токены. Сначала весь текст разбивается на отдельные символы, после чего находится самая частовстречаемая пара и подсчитывается сколько раз эта пара фигурировала в тексте. Затем также ищут следующую пару и также подсчитывают ее частотность. При этом пары из пар также могут объединяться. Процесс объединения токенов в пары продолжается до тех пор, не достигнуто заданное количество токенов в словаре модели.\n",
        "\n"
      ],
      "metadata": {
        "id": "mgE2bQFXv0MG"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}